<!-- Generated by SpecStory -->

# Python binary incompatibility error (2025-06-24 00:26:33)

_**User**_

do the below for the attached chat:


<name>LLM Session Audit & Reflection</name>

Prompt Purpose:
To help users systematically review any LLM chat session—identifying which prompts, techniques, or conversational moves were most effective or ineffective, extracting actionable insights, and building better prompting habits over time.

Context:
You are an advanced prompt engineer and LLM user who wants to maximize the value of your time spent in AI conversations. You regularly engage in business research, coding, troubleshooting, and creative ideation with LLMs. Your main goals are to reduce repetition, identify what works (and what doesn’t), and create a workflow for continuous improvement in prompt engineering. You want a hybrid output: a bullet-point summary of key findings, a structured table for the three most impactful insights, dynamic reflection questions, and a to-do list for next time.

Instruction:
Given the full transcript or excerpt from an LLM session (covering any topic or use case):

1. Audit the conversation to extract:
   - Effective prompts, instructions, or techniques that advanced the conversation or produced useful results.
   - Ineffective or repetitive moves that stalled progress or wasted time.
   - Any meta-communication about preferences, context, or clarification that improved results.

2. Generate a bullet-point summary, clearly separating what worked versus what didn’t—include specific examples.

3. For the three most impactful “moves” (positive or negative), fill out a structured table:
   - Columns: Prompt/Move | Worked? | Why/Why Not | Next Time

4. Dynamically generate 2-4 reflection questions based on the unique flow and pain points of the session (e.g., “Did I clarify my intent up front?” “Were instructions ambiguous?” “Was there context missing that led to confusion?”).

5. End with a concise, actionable to-do/improvement list for your next LLM session.

Output Format:
- Bullet-point summary (what worked/what didn’t; examples)
- Structured table (top 3 findings)
- Dynamic reflection questions (2–4, tailored to session)
- Actionable to-do/improvement list (for next session)



---

_**Assistant**_



---

